TF 2.0 is a C++ engine for mathematics

Flattens an n-tensor into a 1-tensor

more layers == more layers of features
softmax means give me a probability distribution based on the input data

pip install tensorflow-gpu==2.0.0-beta1
import tensorflow as tf
tf.__version__

flow comes from the fact that a dataflow graph is built from your program
in c++, compiles it, and then executes that.

Roughly, TF2 works like numpy, replacing ndarrays with tensors that can be
accelerated on a GPU, and we can backprop through it.
In TF1, you would actually build your dataflow graph and then do:
with tf.Session() as sess:
    print(sess.run(x))
By default in TF2 your code will run in the standard mode, but you can do
@tf.function to make it go real fast (recursively sends all code inside of
it to the tensor back end to be compiled)

Going big: running on multiple GPUs
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = ..
Here the bottleneck is getting your input data off disk and into memory
In TF, the way you can get disk to GPUs quickly is using tf.data (it is not
easy to learn but can remove the latency that comes from disk IOs)

Previously, Keras was effectively an API without an implementation, and could sit on
top of any tensor backend.
from tensorflow.keras import layers
from keras import layers

Keras offers a few APIs
1. Sequential (layered)
2. Functional API (DAG) -- useful for skip connections and residual networks
3. Subclassing API (very similar to chainer and PyTorch) where a model is
   defined by extending a class
    -- In the constructor you define your layers
    -- In the call method, you define the forward pass

All models can be fit by
1. model.fit
2. with tf.GradientTape() as tape:
       pass
here, all computations in the with block are recorded on a graph (tape) and then run backwards
to get the gradients


