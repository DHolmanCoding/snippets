Q: What is the structure and symmetries of my dataset?
==> Is there a model that can exploit these features?

Learning representations of variable length data
RNNs (LSTMs (introduce multiplicative interactions/GRUs/etc))
Downsides:
    - sequential computation
    - no explicit modeling of long/short range dependencies
    - no modeling of hierarchy
    - RNNs w/ sequence aligned states seem wasteful

RNNs consume a sequential input, and at every "timestep" produce a continuous representation
that is a summarizaiton of everything they have consumed up to this point.

CNN
- Trivial to paralellize
- exploits local dependencies
- 'interaction distance' between positions linear or logarithmic
- long-distance dependencies require many layers

Self-attention
- constant 'path length' between two positions
- gating/multiplicative interactions
- trivial to parallelize

attention is position invariant, so position representation must be injected

at a specific position, you can re-express the word at that position as a convex
combination of all the positions
Q: bilinear form

Impose causality by adding highly negative values on the attention logits.

Attention is cheap (FLOPS):
self attn: O(len * len * dim)
RNN: O(len * dim * dim)
CNN: O(len * dim * dim * kernel width)

Attention is like re-expressing each word as a linear combination of
comparisons between the word in question and all other words.

Context/background:
1. classification and regression using self-attention
2. self-attention with RNNs
3. Recurrent attention
