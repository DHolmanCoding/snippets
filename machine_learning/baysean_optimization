https://www.youtube.com/watch?v=C5nqEHpdyoE

optimization involves designing a sequential strategy which maps the collected
data to the next query point.

Proving regret bounds reveals similarity in bayesian/bandit approaches
Both bayesian/bandit approaches are simplified RL problems
- exploration/exploitation trade-off
- final recommendation: pure exploration vs regret minimization vs cumulative regret

Axis of black box techniques:
- online vs offline
- bayesian vs frequentist
- independent vs correlated

Q1: what is my model?
Q2: what is my exploration strategy?
