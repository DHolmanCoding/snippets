https://www.youtube.com/watch?v=C5nqEHpdyoE

optimization involves designing a sequential strategy which maps the collected
data to the next query point.

Proving regret bounds reveals similarity in bayesian/bandit approaches
Both bayesian/bandit approaches are simplified RL problems
- exploration/exploitation trade-off
- final recommendation: pure exploration vs regret minimization vs cumulative regret

Axis of black box techniques:
- online vs offline
- bayesian vs frequentist
- independent vs correlated

Q1: what is my model for latent function, f?
Q2: what is my exploration strategy?

"use a gaussian process" ==> too glib, but they give a general mechanism
for modelling continuous functions

We want a model M that can make predictions and maintain a measure of uncertainty of
those predictions.
types: Universal, thompson (easy to implement), PI (probability of improvement),
EI (Expected improvement), UCB (upper confidence bound algos), (P)ES (predictive
entropy search methods)

